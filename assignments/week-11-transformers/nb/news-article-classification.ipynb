{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "TfzL6S25dL6j",
      "metadata": {
        "id": "TfzL6S25dL6j"
      },
      "source": [
        "<p align = \"center\" draggable=”false” ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
        "     width=\"200px\"\n",
        "     height=\"auto\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6727ba",
      "metadata": {
        "id": "cd6727ba"
      },
      "source": [
        "# News Article Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd0ce8d",
      "metadata": {
        "id": "1dd0ce8d"
      },
      "source": [
        "Today, you are a machine learning engineer at [Upday](https://www.upday.com/home), a news app. The engine behind the app processes almost 100k news articles every day in many languages and context. In order to connect people with the right content, we need to know what the articles are about. \n",
        "\n",
        "The job here is to build a classifier that identify the category of an article accurately that will be fed into the recommendation algorithms of the app for better personalized content for the readers. \n",
        "\n",
        "You will first train a Transformer from scratch; then fine-tune a pre-trained Transformer model for text classification using 🤗; and compare performances using the same test data set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c979a92b",
      "metadata": {
        "id": "c979a92b"
      },
      "source": [
        "## Learning objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f6bcf51",
      "metadata": {
        "id": "4f6bcf51"
      },
      "source": [
        "By the end of this session, you will be able to:\n",
        "- Understand how Transformer models work\n",
        "- Build a Transformer model for text classification from scratch\n",
        "- Fine-tune a pre-trained Transformer model for text classification using 🤗 "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64724518",
      "metadata": {
        "id": "64724518"
      },
      "source": [
        "# Task 1. Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a57962",
      "metadata": {
        "id": "48a57962"
      },
      "source": [
        "1. [20 news groups data](http://qwone.com/~jason/20Newsgroups/) is one of the standard datasets in the `scikit-learn`. The 20 newsgroups dataset comprises around 18k newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the two datasets is based upon messages posted before and after a specific date.\n",
        "\n",
        "    Take a read on its [usage](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset) and load the training and testing datasets into `train` and `test` respectively via `fetch_20newsgroups`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f4d54b2",
      "metadata": {
        "id": "1f4d54b2"
      },
      "outputs": [],
      "source": [
        "NUM_LABELS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511c1fd2",
      "metadata": {
        "id": "511c1fd2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# YOUR CODE HERE\n",
        "train = fetch_20newsgroups(subset=\"train\")\n",
        "test  = fetch_20newsgroups(subset=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0ed320",
      "metadata": {
        "id": "dd0ed320"
      },
      "source": [
        "1. Explore the data. For example, what is the data type of `train` and `test`? What is the data type of `train['data']`? What is in `train['target_names']`? \n",
        "\n",
        "    YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "685b52dd",
      "metadata": {
        "id": "685b52dd"
      },
      "source": [
        "1. Train / validation / test splits. \n",
        "\n",
        "    There are four keys in each data set and we need \"data\" (the text) and \"target\" (the category). Extract the \"data\" and create a `pd.Series` named `X`; and extract the \"target\" and create a `pd.Series` named `y`. Do the same for the `test` data set, named `X_test` and `y_test`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a044fbf",
      "metadata": {
        "id": "4a044fbf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# YOUR CODE HERE\n",
        "X, y = train.data, train.target\n",
        "X_test, y_test = test.data, test.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad6b5518",
      "metadata": {
        "id": "ad6b5518"
      },
      "source": [
        "Now split `X` and `y` into training and validation sets. Use `train_test_split` from `sklearn.model_selection` to split the data; save 10% of the data for validation and set the random state to 19."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b792fff",
      "metadata": {
        "id": "5b792fff"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# YOUR CODE HERE\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.9, random_state=19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b97380",
      "metadata": {
        "id": "17b97380"
      },
      "outputs": [],
      "source": [
        "assert y_train.shape == (10182,)\n",
        "assert y_valid.shape == (1132,)\n",
        "assert y_test.shape  == (7532,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e3d71a0",
      "metadata": {
        "id": "8e3d71a0"
      },
      "source": [
        "1. Convert categorical labels into dummy variables. \n",
        "\n",
        "    Use `pd.get_dummies` to convert labels into dummy variables for `y_train`, `y_valid`, `y_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e335c2",
      "metadata": {
        "id": "72e335c2"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "y_train = pd.get_dummies(y_train) \n",
        "y_valid = pd.get_dummies(y_valid)\n",
        "y_test  = pd.get_dummies(y_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3a80ca",
      "metadata": {
        "id": "ee3a80ca"
      },
      "outputs": [],
      "source": [
        "assert y_train.shape == (10182, NUM_LABELS)\n",
        "assert y_valid.shape == (1132, NUM_LABELS)\n",
        "assert y_test.shape  == (7532, NUM_LABELS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9603eb10",
      "metadata": {
        "id": "9603eb10"
      },
      "source": [
        "\n",
        "# Task 2. Attention is all you need"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c39db15",
      "metadata": {
        "id": "9c39db15"
      },
      "source": [
        "Recently most of the natural language processing tasks are being dominated by the `Transformer` architecture, introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762), which used a simple mechanism called `Neural Attention` as one of its building blocks. As the title suggests this architecture didn't require any recurrent layer. We now build a text classification using Attention and Positional Embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb0fa502",
      "metadata": {
        "id": "fb0fa502"
      },
      "source": [
        "1. Transformer (attention) Block. \n",
        "\n",
        "    The concept of `Neural Attention` is fairly simple; i.e., not all input information seen by a model is equally important to the task at hand. Although this concept has been utilized at various different places as well, e.g., max pooling in ConvNets, but the kind of attention we are looking for should be `context aware`.\n",
        "\n",
        "    The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other; in other words, calculate attention of all other inputs with respect to one input.\n",
        "\n",
        "    In the paper, the authors proposed another type of attention mechanism called multi-headed attention which refers to the fact that the outer space of the self attention layer gets factored into a set of independent sub-spaces learned separately, where each subspace is called a \"head\". **You need to implement the multi-head attention layer**, supplying values for two parameters: `num_heads` and `key_dim`.\n",
        "\n",
        "    There is a learnable dense projection present after the multi-head attention which enables the layer to actually learn something, as opposed to being a purely stateless transformation. **You need to implement `dense_proj`**, use the `tf.keras.Sequential` to stack two dense layers: \n",
        "    \n",
        "        1. first dense layer with `dense_dim` units and activation function `relu`;\n",
        "        2. second dense layer with `embed_dim` units and no activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc578b76",
      "metadata": {
        "id": "cc578b76"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers as L\n",
        "from tensorflow import keras\n",
        "\n",
        "class TransformerBlock(L.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        # YOUR CODE HERE\n",
        "        self.attention = L.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        # YOUR CODE HERE\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            L.Dense(dense_dim, activation=\"relu\"),\n",
        "            L.Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = L.LayerNormalization()\n",
        "        self.layernorm2 = L.LayerNormalization()\n",
        "        super().__init__(**kwargs)\n",
        "    \n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[: tf.newaxis, :]\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm2(proj_input + proj_output)\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super().get_confog()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1cb0ce3",
      "metadata": {
        "id": "e1cb0ce3"
      },
      "source": [
        "1. Positional embedding. \n",
        "\n",
        "    The idea behind Positional Encoding is fairly simple as well: to give the model access to token order information, therefore we are going to add the token's position in the sentence to each word embedding.\n",
        "\n",
        "    Thus, one input word embedding will have two components: the usual token vector representing the token independent of any specific context, and a position vector representing the position of the token in the current sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db9b3dd",
      "metadata": {
        "id": "2db9b3dd"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(L.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        # YOUR CODE HERE\n",
        "        self.token_embeddings = keras.layers.Embedding(input_dim=input_dim, output_dim=output_dim)(inputs)\n",
        "        # YOUR CODE HERE\n",
        "        self.position_embeddings = keras.layers.PositionEmbedding(sequence_length=sequence_length)(self.token_embeddings)\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afde9c93",
      "metadata": {
        "id": "afde9c93"
      },
      "source": [
        "1. Define some constants to parameterize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "280fe4c7",
      "metadata": {
        "id": "280fe4c7"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10_000\n",
        "EMBED_DIM = 256\n",
        "DENSE_DIM = 32\n",
        "NUM_HEADS = 2\n",
        "MAX_LEN = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ecb3a8d",
      "metadata": {
        "id": "9ecb3a8d"
      },
      "source": [
        "1. Tokenizer. \n",
        "    \n",
        "    The tokenizer is a simple tool to convert a text into a sequence of tokens. It is used to convert the training data into a sequence of integers, which are then used as input to the model. \n",
        "\n",
        "    Use `Tokenizer` to create a tokenizer for the training data. Set the `num_words` parameter to the number of words to keep in the vocabulary, and `oov_token` to be `\"<unk>\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17d66cfc",
      "metadata": {
        "id": "17d66cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "cb32436e-fdc5-4a9a-a436-8b3230d7335a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-f86c5aa83734>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    tokenizer = # YOUR CODE HERE\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "Lfrom keras.preprocessing.text import Tokenizer\n",
        "tokenizer = # YOUR CODE HERE\n",
        "tokenizer.fit_on_texts(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab4b7a0",
      "metadata": {
        "id": "eab4b7a0"
      },
      "source": [
        "1. Pad the sequences. \n",
        "    \n",
        "    The tokenizer outputs a sequence of integers, which are then used as input to the model. However, the model expects a sequence of fixed length. To pad the sequences to the same length, use `sequence.pad_sequences` from `keras.preprocessing`.\n",
        "\n",
        "    Complete function `preprocess` below to 1) tokenize the texts 2) pad the sequences to the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5GyEyhfuDybK",
      "metadata": {
        "id": "5GyEyhfuDybK"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import sequence\n",
        "\n",
        "def preprocess(texts, tokenizer, maxlen:int = MAX_LEN):\n",
        "    seqs = # YOUR CODE HERE\n",
        "    tokenized_text = # YOUR CODE HERE\n",
        "    return tokenized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f4f3291",
      "metadata": {
        "id": "8f4f3291"
      },
      "source": [
        "1. Preprocess the data. \n",
        "    \n",
        "    Use `preprocess` to preprocess the training, validation, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041570a5",
      "metadata": {
        "id": "041570a5"
      },
      "outputs": [],
      "source": [
        "X_train = # YOUR CODE HERE\n",
        "X_valid = # YOUR CODE HERE\n",
        "X_test  = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f65487a",
      "metadata": {
        "id": "6f65487a"
      },
      "source": [
        "1. Define the model with the following architecture: \n",
        "\n",
        "    * Input Layer\n",
        "    * Positional Embeddings\n",
        "    * Transformer Block\n",
        "    * Pooling\n",
        "    * Dropout\n",
        "    * Output Layer\n",
        "\n",
        "    If you are not familiar with keras functional API, take a read [here](https://keras.io/guides/functional_api/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "141d4968",
      "metadata": {
        "id": "141d4968"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
        "x = # YOUR CODE HERE\n",
        "x = # YOUR CODE HERE\n",
        "x = L.GlobalMaxPooling1D()(x)\n",
        "x = L.Dropout(0.5)(x)\n",
        "outputs = L.Dense(20, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7329eef2",
      "metadata": {
        "id": "7329eef2"
      },
      "source": [
        "1. Compile model. \n",
        "\n",
        "    Use 'adam' for the optimizer and accuracy for metrics, supply the correct value for loss. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6491748",
      "metadata": {
        "id": "f6491748"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=# YOUR CODE HERE\n",
        "    loss=# YOUR CODE HERE\n",
        "    metrics=# YOUR CODE HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c00f20",
      "metadata": {
        "id": "d1c00f20"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZN4agZCL7E6e",
      "metadata": {
        "id": "ZN4agZCL7E6e"
      },
      "source": [
        "1. Add [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) and [ReduceLROnPlateau](https://keras.io/api/callbacks/reduce_lr_on_plateau/) to stop training if the model does not improve a set metric after a given number of epochs. \n",
        "\n",
        "    Create an `EarlyStopping` object named `es` to stop training if the validation loss does not improve after 5 epochs. Set verbose to display messages when the callback takes an action and set `restore_best_weights` to restore model weights from the epoch with the best value of the monitored metric.\n",
        "    \n",
        "    Use `ReduceLROnPlateau` to reduce the learning rate if the validation loss does not improve after 3 epochs. Set verbose to display messages when the callback takes an action and use default values for other parameters.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220efe0c",
      "metadata": {
        "id": "220efe0c"
      },
      "outputs": [],
      "source": [
        "es = # YOUR CODE HERE\n",
        "rlp = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c75279",
      "metadata": {
        "id": "58c75279"
      },
      "source": [
        "1. Train the model. \n",
        "    \n",
        "    Supply both `EarlyStopping` and `ReduceLROnPlateau` for `callbacks`. Set `epochs` to 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c8d14d0",
      "metadata": {
        "id": "1c8d14d0"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train, \n",
        "    validation_data=(X_valid, y_valid),\n",
        "    callbacks= # YOUR CODE HERE\n",
        "    epochs=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd54e08",
      "metadata": {
        "id": "4cd54e08"
      },
      "source": [
        "1. Evaluate the trained model on the test data. \n",
        "    \n",
        "    Use `evaluate` to evaluate the model on the test data (accuracy will be around 0.77)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HCVwNQ0UqH_x",
      "metadata": {
        "id": "HCVwNQ0UqH_x"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f3bc7bf",
      "metadata": {
        "id": "3f3bc7bf"
      },
      "source": [
        "1. Visualize both loss and accuracy curves for the training and validation data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20fdf465",
      "metadata": {
        "id": "20fdf465"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e_aWlv8RYI-4",
      "metadata": {
        "id": "e_aWlv8RYI-4"
      },
      "source": [
        "# Task 3. Fine-tune pre-trained model via 🤗"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5490667e",
      "metadata": {
        "id": "5490667e"
      },
      "source": [
        "Substantial gains can be achieved by pre-training on a large corpus of text followed by fine-tuning a specific task. In this task, we will take advantage of [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf), [`distillbert-uncased`](https://huggingface.co/distilbert-base-uncased) specifically, and fine-tune it on the 20 Newsgroups dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b24afd2f",
      "metadata": {
        "id": "b24afd2f"
      },
      "source": [
        "1. Install [`Transformers`](https://huggingface.co/docs/transformers) and [`datasets`](https://huggingface.co/docs/datasets/index)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "st1OGgI4HmqS",
      "metadata": {
        "id": "st1OGgI4HmqS"
      },
      "outputs": [],
      "source": [
        "! pip install -U -q transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vC5O5FiWYPHe",
      "metadata": {
        "id": "vC5O5FiWYPHe"
      },
      "source": [
        "1. Import the library and check its version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "maGzeTY8Ht85",
      "metadata": {
        "id": "maGzeTY8Ht85"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc75685",
      "metadata": {
        "id": "0dc75685"
      },
      "source": [
        "1. Create `Dataset` objects for train / validation / test sets that are better compatible with the `Transformers` API.\n",
        "\n",
        "    You can first create a `pd.DataFrame` with two fields: `text` and `label` with `data` and `target` from `train`. Then, call the `Dataset.from_pandas()` method to create a `Dataset` object and save it to `train_ds`. \n",
        "\n",
        "    Do the same for test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RnLjbcxvHl7u",
      "metadata": {
        "id": "RnLjbcxvHl7u"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "train_ds = # YOUR CODE HERE\n",
        "test_ds  = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6699275d",
      "metadata": {
        "id": "6699275d"
      },
      "source": [
        "1. Cast column `label` as `datasets.features.ClassLabel` object using [`class_encode_column`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.class_encode_column) for both `train_ds` and `test_ds`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wwmqiDg8H9W7",
      "metadata": {
        "id": "wwmqiDg8H9W7"
      },
      "outputs": [],
      "source": [
        "train_ds = # YOUR CODE HERE\n",
        "test_ds = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fa82ebb",
      "metadata": {
        "id": "1fa82ebb"
      },
      "source": [
        "1. Create the validation set by apply [`train_ds.train_test_split()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.train_test_split). Similarly, set the test size to 0.1, and set the random state to 19. Make sure the split is stratified by column `label`. Save the result to `train_dsd` as the result is a DatasetDict object.\n",
        "\n",
        "    Here's another way to do train / validation / test split: [ref](https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VvDkzSZsIEHx",
      "metadata": {
        "id": "VvDkzSZsIEHx"
      },
      "outputs": [],
      "source": [
        "train_dsd = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o3XIsj3Z94wi",
      "metadata": {
        "id": "o3XIsj3Z94wi"
      },
      "outputs": [],
      "source": [
        "train_dsd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf61e97c",
      "metadata": {
        "id": "bf61e97c"
      },
      "source": [
        "1. Yet the 'test' set inside `train_dsd` is meant for validation, the following code will rename the test set to validation and add a new DataSet object for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zwq78iZiIUxE",
      "metadata": {
        "id": "Zwq78iZiIUxE"
      },
      "outputs": [],
      "source": [
        "train_dsd['validation'] = train_dsd['test']\n",
        "train_dsd['test'] = test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AkW4gXlqIhdq",
      "metadata": {
        "id": "AkW4gXlqIhdq"
      },
      "outputs": [],
      "source": [
        "train_dsd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ab80c71",
      "metadata": {
        "id": "8ab80c71"
      },
      "source": [
        "1. Load the DistilBERT tokenizer to process the text. \n",
        "    \n",
        "    Use [`AutoTokenizer`](https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/auto#transformers.AutoTokenizer) to load the tokenizer for the given `model_checkpoint`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xhq4BycpLjwj",
      "metadata": {
        "id": "Xhq4BycpLjwj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a26945",
      "metadata": {
        "id": "d0a26945"
      },
      "source": [
        "1. Create a preprocessing function to tokenize text, truncate and pad sequences to be no longer than DistilBERT’s maximum input length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6hnc5JOejihx",
      "metadata": {
        "id": "6hnc5JOejihx"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=MAX_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8cc46b6",
      "metadata": {
        "id": "f8cc46b6"
      },
      "source": [
        "1. Use 🤗 Datasets [map](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map) function to apply the preprocessing function over the entire dataset. You can speed up the map function by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6KWmC63UImNZ",
      "metadata": {
        "id": "6KWmC63UImNZ"
      },
      "outputs": [],
      "source": [
        "tokenized_text = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3WvXKDqENoqa",
      "metadata": {
        "id": "3WvXKDqENoqa"
      },
      "outputs": [],
      "source": [
        "tokenized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78e1e59d",
      "metadata": {
        "id": "78e1e59d"
      },
      "source": [
        "1. Use [DataCollatorWithPadding](https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding) to create a batch of examples. Set the type of Tensor to return as 'tf' as we will fine-tune the model in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D8xk28M1O4GO",
      "metadata": {
        "id": "D8xk28M1O4GO"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0fe020",
      "metadata": {
        "id": "bc0fe020"
      },
      "source": [
        "1. To fine-tune a model in TensorFlow, start by converting datasets to the `tf.data.Dataset` format with `to_tf_dataset`. \n",
        "\n",
        "    Specify inputs and labels in columns, whether to shuffle the dataset order, batch size, and the data collator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25fh9ZxkIsI4",
      "metadata": {
        "id": "25fh9ZxkIsI4"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "tf_train_set = tokenized_text[\"train\"].to_tf_dataset(\n",
        "    columns=# YOUR CODE HERE\n",
        "    shuffle=# YOUR CODE HERE\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "tf_validation_set = tokenized_text[\"validation\"].to_tf_dataset(\n",
        "    columns=# YOUR CODE HERE\n",
        "    shuffle=# YOUR CODE HERE\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        "    )\n",
        "tf_test_set = tokenized_text[\"test\"].to_tf_dataset(\n",
        "    columns=# YOUR CODE HERE\n",
        "    shuffle=# YOUR CODE HERE\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=data_collator,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e475a9f9",
      "metadata": {
        "id": "e475a9f9"
      },
      "source": [
        "1. Set up an optimizer function, learning rate schedule, and some training hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ZJblOkpW27x",
      "metadata": {
        "id": "6ZJblOkpW27x"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "EPOCHS = 5\n",
        "batches_per_epoch = len(tokenized_text[\"train\"]) // BATCH_SIZE\n",
        "total_train_steps = int(batches_per_epoch * EPOCHS)\n",
        "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2997fc2",
      "metadata": {
        "id": "f2997fc2"
      },
      "source": [
        "1. Load DistilBERT with  [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.20.1/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification) along with the number of expected labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vCnl8FHEYanJ",
      "metadata": {
        "id": "vCnl8FHEYanJ"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "my_bert = # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97cb0e7b",
      "metadata": {
        "id": "97cb0e7b"
      },
      "source": [
        "1. Don't forget to configure the model for training with [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s7HGL_nCYfba",
      "metadata": {
        "id": "s7HGL_nCYfba"
      },
      "outputs": [],
      "source": [
        "my_bert.compile(optimizer=optimizer,  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QifyGkqhofwB",
      "metadata": {
        "id": "QifyGkqhofwB"
      },
      "source": [
        "1. Finally, let the fine-tuning start!\n",
        "\n",
        "    You may run into CoLab's usage limit, feel free to lower the parameter `epochs` to 3; even so, it could take ~10 minutes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cJjfKl0TY0A0",
      "metadata": {
        "id": "cJjfKl0TY0A0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vPEoQlMw5VBq",
      "metadata": {
        "id": "vPEoQlMw5VBq"
      },
      "source": [
        "1. As you are waiting for the fine-tuning to finish, how many parameters does DistilBERT have? How about BERT? What is the world's largest open Multilingual Language Model as of July 2022? (hint: 🌸) \n",
        "\n",
        "    YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4b23c02",
      "metadata": {
        "id": "c4b23c02"
      },
      "source": [
        "1. Evaluate the model on the test data. \n",
        "    \n",
        "    Use `evaluate` to evaluate the model on the test data (accuracy shall be around 0.84)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vyvGP0mY-4_x",
      "metadata": {
        "id": "vyvGP0mY-4_x"
      },
      "outputs": [],
      "source": [
        "bert_loss, bert_acc = my_bert.evaluate(tf_test_set) # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba3081e8",
      "metadata": {
        "id": "ba3081e8"
      },
      "source": [
        "1. Optional. If you are happy with the result, you can save the model to a file. \n",
        "    \n",
        "    One easy way to do it is to use [`save_pretrained()`](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.save_pretrained) to save the model to a file. Check [Export 🤗 Transformers Models](https://huggingface.co/docs/transformers/serialization) for deployment options."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uD3fV0qVn8qv",
      "metadata": {
        "id": "uD3fV0qVn8qv"
      },
      "source": [
        "# Acknowledgement & Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TqTjj4qRoApb",
      "metadata": {
        "id": "TqTjj4qRoApb"
      },
      "source": [
        "- Part of the notebook is adapted from [Text Classification - Attention](https://www.kaggle.com/code/ritvik1909/text-classification-attention)\n",
        "- Part of the notebook is adapted from [Fine-tune a pretrained model with Hugging Face](https://huggingface.co/docs/transformers/training)\n",
        "- We used accuracy for metric, check out [Comprehensive Guide to Multiclass Classification Metrics](https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd).\n",
        "- [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n",
        "- [Hugging Face: State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0](https://medium.com/tensorflow/using-tensorflow-2-for-state-of-the-art-natural-language-processing-102445cda54a)\n",
        "- More readings on Transformers, check the Course Materials section on [CS25: Transformers United](https://web.stanford.edu/class/cs25/)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 126.733939,
      "end_time": "2022-03-31T16:26:20.094746",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-03-31T16:24:13.360807",
      "version": "2.3.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "eb5849a166af78ad4c01ecae3786b95da7c29bbfa89231c0b15e21ad464a060f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}